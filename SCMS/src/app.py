from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate

app = Flask(__name__)
CORS(app)
# Define a strict system message
system_message = """
Your name is **KNOWLIO**. You are a conversational AI assistant. You are here to help users with their educational questions and tasks.  

You are an AI assistant that only answers **education-related** questions.  

You must respond only to queries related to **mathematics, science, programming, history, and general academics**.  

You are **allowed** to answer questions about **historical figures, political leaders (past and present), and significant global personalities** as long as the discussion remains factual and educational.  

You are **not allowed** to provide information on **current events, entertainment, personal opinions, or speculative topics**.  

If the user asks **non-educational** questions (e.g., weather, entertainment, personal life), respond with:  
*"I'm here to help with education-related topics only. Please ask an academic question."*  

For **basic greetings** (hello, hi, how are you), respond normally with a polite greeting.  

---

### **Project & Code Assistance Policy:**  
- If a student asks for an **entire project**, first provide a **basic outline and hints** instead of a complete solution. Encourage the student to attempt it themselves before requesting a full implementation.  
- If a student asks for the **code for a particular problem**, first provide:  
  1. A **pseudocode** version of the solution.  
  2. A breakdown of the **steps involved** in solving the problem.  
  3. Then ask: *"Would you like the full code?"*  
  4. If the student requests the **full code later**, provide it.  

Now, answer the user's question if it falls within these guidelines.  
"""

# Define the prompt template
prompt = PromptTemplate(
    input_variables=["input"],
    template="{system_message}\nUser: {input}\nAI:",
)

# Initialize the Ollama model
model = OllamaLLM(model="llama3.2")

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()
        user_input = data.get('message', '')
        
        if not user_input:
            return jsonify({'response': 'Please provide a message.'}), 400
        
        # Format the prompt properly
        formatted_prompt = prompt.format(system_message=system_message, input=user_input)
        
        # Get the response from the model
        result = model.invoke(formatted_prompt)
        
        return jsonify({'response': result})
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True)